{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN TOPIC MODELLING PIPELINE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pre_Processing import *\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"december_2017.csv\")\n",
    "df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET THE MAX AND MIN DATES IN TEXTUAL DATA\n",
    "\n",
    "MIN_DATE = str(min(df[\"timestamp\"])).split(' ')[0]\n",
    "MAX_DATE = str(max(df[\"timestamp\"])).split(' ')[0]\n",
    "\n",
    "\n",
    "#GET THE CORRESPONDING STEEM PRIC\n",
    "\n",
    "price_data = pd.read_csv(\"SteemCoinFull.csv\",sep=\";\")\n",
    "price_data[\"Date\"] = pd.to_datetime(price_data[\"Date\"])\n",
    "\n",
    "price_data = price_data[(price_data[\"Date\"]>=MIN_DATE) & (price_data[\"Date\"]<=MAX_DATE)]\n",
    "price_data = price_data.sort_values(by=\"Date\")\n",
    "price_data = price_data.reset_index()\n",
    "price_data = price_data.drop(\"index\",axis=1)\n",
    "price_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN, DETECT LANGUAGE AND GET ONLY ENGLISH ONES\n",
    "\n",
    "df = pre_processing(df,\"body\",\"cleaned_text\")\n",
    "\n",
    "#TOKENIZE, REMOVE STOPWORDS, CREATE BI-GRAMS AND LEMMATIZED\n",
    "\n",
    "lemmatized = prepare_to_LDA(df,\"cleaned_text\",tokenizer=sent_to_words, remove_stopwords = remove_stopwords, make_bigrams = make_bigrams,\n",
    "                  lemmatization = lemmatization)\n",
    "\n",
    "# RUN TOPIC MODELLING\n",
    "\n",
    "topics,coherence_values, model_list,corpus,id2word = modelling_LDAmallet(lemmatized, n_topics=[8,10,12,14,16,18], random_state=10, chunksize=100, passes=5,\n",
    "                                              model=\"MALLET\")\n",
    "\n",
    "n_topics=[8,10,12,14,16,18]\n",
    "\n",
    "max_coh = max(coherence_values)\n",
    "\n",
    "model_max_ind = coherence_values.index(max_coh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT COHERENCE VALUES\n",
    "\n",
    "plot_coh_val(n_topics,coherence_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE TOPICS\n",
    "\n",
    "from pyLDAvis import gensim as ge\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = ge.prepare(model_list[1], corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ASSIGN TOPICS, CALCULATE DAILY GROWTH OF THE TOPICS AND STEEM PRICE\n",
    "\n",
    "df_dominant_topic = format_topics_sentences(model_list[1],corpus,texts=df[\"cleaned_text\"].values.tolist())\n",
    "\n",
    "daily_topics = calculate_dailytopic_growth(df,df_dominant_topic)\n",
    "\n",
    "steem_daily_growth = steem_growth(price_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SEE THE TOPICS AND KEYWORDS\n",
    "\n",
    "get_topics(df_dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE THE RANGE YOU LIKE AND CALCULATE CORRELATION BETWEEN TOPIC GROWTH AND STEEM PRICE\n",
    "\n",
    "MIN_DATE = \"2017-12-01\"\n",
    "MAX_DATE = \"2017-12-31\"\n",
    "\n",
    "corr_per_topic,corr_df = calculate_correlation(daily_topics,steem_daily_growth,MIN_DATE,MAX_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPUTE CROSS CORRELATION\n",
    "\n",
    "compute_cross_cor(corr_per_topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLARITY CALCULATION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOIN TIMESTAMP AND ASSIGNED TOPICS\n",
    "\n",
    "clean_df = pd.concat([df[[\"timestamp\",\"body\",\"cleaned_text\"]],df_dominant_topic.Dominant_Topic],axis=1)\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CHOOSE THE TOPICS YOU LIKE HERE. IF NOT PRESENT, CREATE ONE.\n",
    "\n",
    "first_topic = 8\n",
    "second_topic = 9\n",
    "third_topic = 12\n",
    "fourth_topic = 13\n",
    "\n",
    "## IF ADDED A NEW TOPIC, PLEASE MODIFY THE CODE BELOW ACCORDINGLY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_polarity = clean_df[(clean_df[\"Dominant_Topic\"]==first_topic) | \n",
    "                            (clean_df[\"Dominant_Topic\"]==second_topic) | (clean_df[\"Dominant_Topic\"] == third_topic) | \n",
    "                            (clean_df[\"Dominant_Topic\"] == fourth_topic)][[\"timestamp\",\"body\",\"cleaned_text\",\"Dominant_Topic\"]].copy()\n",
    "data_to_polarity = data_to_polarity.reset_index().drop(\"index\",axis=1).sort_values(by=\"timestamp\")\n",
    "\n",
    "data_to_polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD PRE-TRAINED MODEL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(main_path+\"/twitter_model\")\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "loaded = load_model('model.h5')\n",
    "import tqdm\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "\n",
    "kv_model = KeyedVectors.load('model.w2v', mmap='r')\n",
    "\n",
    "import pickle\n",
    "encoder = open('encoder.pkl', 'rb')    \n",
    "pickle.load(encoder)\n",
    "\n",
    "\n",
    "import json\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "\n",
    "with open('tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    \n",
    "    \n",
    "def predict(text, include_neutral=True):\n",
    "    start_at = time.time()\n",
    "    # Tokenize text\n",
    "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
    "    # Predict\n",
    "    score = loaded.predict([x_test])[0]\n",
    "    # Decode sentiment\n",
    "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
    "\n",
    "    return {\"label\": label, \"score\": float(score),\n",
    "       \"elapsed_time\": time.time()-start_at}  \n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "SEQUENCE_LENGTH = 300\n",
    "\n",
    "\n",
    "POSITIVE = \"POSITIVE\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
    "\n",
    "#PREDICT\n",
    "\n",
    "def decode_sentiment(score, include_neutral=True):\n",
    "    if include_neutral:        \n",
    "        label = NEUTRAL\n",
    "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
    "            label = NEGATIVE\n",
    "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
    "            label = POSITIVE\n",
    "\n",
    "        return label\n",
    "    else:\n",
    "        return NEGATIVE if score < 0.5 else POSITIVE\n",
    "    \n",
    "    \n",
    "    \n",
    "def calculate_polarity(data_to_polarity):\n",
    "    \n",
    "    import tqdm\n",
    "    \n",
    "    res_dict = {}\n",
    "    text = data_to_polarity.cleaned_text.tolist()\n",
    "    \n",
    "    for index in tqdm.tqdm(range(len(text))):\n",
    "        \n",
    "        res = predict(text[index])[\"score\"]\n",
    "        res_dict[index] = res\n",
    "    \n",
    "    result = pd.DataFrame.from_dict(res_dict,orient=\"index\")\n",
    "    result = result.rename({0:\"score\"},axis=1)\n",
    "    data_to_polarity[\"score\"] = result[\"score\"].copy()\n",
    "    \n",
    "    data_to_polarity[\"timestamp\"] =pd.to_datetime(data_to_polarity[\"timestamp\"].apply(lambda row: row.split(' ')[0]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    return data_to_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm\n",
    "\n",
    "polarity_result = calculate_polarity(data_to_polarity)\n",
    "polarity_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
